Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/11/22 03:52:42 INFO SparkContext: Running Spark version 2.0.0
20/11/22 03:52:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/22 03:52:44 INFO SecurityManager: Changing view acls to: prosenjitdas
20/11/22 03:52:44 INFO SecurityManager: Changing modify acls to: prosenjitdas
20/11/22 03:52:44 INFO SecurityManager: Changing view acls groups to: 
20/11/22 03:52:44 INFO SecurityManager: Changing modify acls groups to: 
20/11/22 03:52:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(prosenjitdas); groups with view permissions: Set(); users  with modify permissions: Set(prosenjitdas); groups with modify permissions: Set()
20/11/22 03:52:46 INFO Utils: Successfully started service 'sparkDriver' on port 64365.
20/11/22 03:52:46 INFO SparkEnv: Registering MapOutputTracker
20/11/22 03:52:46 INFO SparkEnv: Registering BlockManagerMaster
20/11/22 03:52:46 INFO DiskBlockManager: Created local directory at /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/blockmgr-b092d263-9e39-4464-8a76-3d5f80ac11f0
20/11/22 03:52:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/11/22 03:52:46 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/22 03:52:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/22 03:52:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.10:4040
20/11/22 03:52:47 INFO Utils: Copying /Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/configs/etl_config.json to /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/spark-257ecfa2-0753-4c68-972b-9b7f5b2e5830/userFiles-f89477df-dd0b-4e57-b1bb-3970b4f0c3e7/etl_config.json
20/11/22 03:52:48 INFO SparkContext: Added file file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/configs/etl_config.json at file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/configs/etl_config.json with timestamp 1606045967735
20/11/22 03:52:48 INFO Utils: Copying /Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/jobs/hellofresh.py to /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/spark-257ecfa2-0753-4c68-972b-9b7f5b2e5830/userFiles-f89477df-dd0b-4e57-b1bb-3970b4f0c3e7/hellofresh.py
20/11/22 03:52:48 INFO SparkContext: Added file file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/jobs/hellofresh.py at file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/jobs/hellofresh.py with timestamp 1606045968094
20/11/22 03:52:48 INFO Executor: Starting executor ID driver on host localhost
20/11/22 03:52:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64366.
20/11/22 03:52:48 INFO NettyBlockTransferService: Server created on 192.168.1.10:64366
20/11/22 03:52:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.10, 64366)
20/11/22 03:52:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.10:64366 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.10, 64366)
20/11/22 03:52:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.10, 64366)
20/11/22 03:52:49 INFO SharedState: Warehouse path is 'file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/spark-warehouse'.
20/11/22 03:52:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 265.0 KB, free 366.0 MB)
20/11/22 03:52:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 366.0 MB)
20/11/22 03:52:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.10:64366 (size: 23.3 KB, free: 366.3 MB)
20/11/22 03:52:55 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:-2
20/11/22 03:52:56 INFO FileInputFormat: Total input paths to process : 1
20/11/22 03:52:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:-2
20/11/22 03:52:57 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:-2) with 2 output partitions
20/11/22 03:52:57 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:-2)
20/11/22 03:52:57 INFO DAGScheduler: Parents of final stage: List()
20/11/22 03:52:57 INFO DAGScheduler: Missing parents: List()
20/11/22 03:52:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at json at NativeMethodAccessorImpl.java:-2), which has no missing parents
20/11/22 03:52:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 366.0 MB)
20/11/22 03:52:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 366.0 MB)
20/11/22 03:52:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.10:64366 (size: 2.5 KB, free: 366.3 MB)
20/11/22 03:52:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
20/11/22 03:52:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at NativeMethodAccessorImpl.java:-2)
20/11/22 03:52:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
20/11/22 03:53:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5671 bytes)
20/11/22 03:53:00 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5671 bytes)
20/11/22 03:53:00 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
20/11/22 03:53:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/22 03:53:00 INFO Executor: Fetching file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/jobs/hellofresh.py with timestamp 1606045968094
20/11/22 03:53:00 INFO Utils: /Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/jobs/hellofresh.py has been previously copied to /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/spark-257ecfa2-0753-4c68-972b-9b7f5b2e5830/userFiles-f89477df-dd0b-4e57-b1bb-3970b4f0c3e7/hellofresh.py
20/11/22 03:53:01 INFO Executor: Fetching file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/configs/etl_config.json with timestamp 1606045967735
20/11/22 03:53:01 INFO Utils: /Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/configs/etl_config.json has been previously copied to /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/spark-257ecfa2-0753-4c68-972b-9b7f5b2e5830/userFiles-f89477df-dd0b-4e57-b1bb-3970b4f0c3e7/etl_config.json
20/11/22 03:53:01 INFO HadoopRDD: Input split: file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/data/recipes.json:449008+449008
20/11/22 03:53:01 INFO HadoopRDD: Input split: file:/Users/prosenjitdas/Documents/Spark/Assignments/prosenjitj-data-engineering-test/data/recipes.json:0+449008
20/11/22 03:53:01 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
20/11/22 03:53:01 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
20/11/22 03:53:01 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
20/11/22 03:53:01 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
20/11/22 03:53:01 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
20/11/22 03:53:04 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1995 bytes result sent to driver
20/11/22 03:53:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1995 bytes result sent to driver
20/11/22 03:53:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4999 ms on localhost (1/2)
20/11/22 03:53:04 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4448 ms on localhost (2/2)
20/11/22 03:53:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/22 03:53:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:-2) finished in 5.375 s
20/11/22 03:53:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:-2, took 7.659155 s
20/11/22 03:53:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.10:64366 in memory (size: 2.5 KB, free: 366.3 MB)
20/11/22 03:53:10 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.10:64366 in memory (size: 23.3 KB, free: 366.3 MB)
Exception on Extracting Data
No Data Present
20/11/22 03:53:11 INFO SparkContext: Invoking stop() from shutdown hook
20/11/22 03:53:12 INFO SparkUI: Stopped Spark web UI at http://192.168.1.10:4040
20/11/22 03:53:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/22 03:53:12 INFO MemoryStore: MemoryStore cleared
20/11/22 03:53:12 INFO BlockManager: BlockManager stopped
20/11/22 03:53:12 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/22 03:53:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/22 03:53:12 INFO SparkContext: Successfully stopped SparkContext
20/11/22 03:53:12 INFO ShutdownHookManager: Shutdown hook called
20/11/22 03:53:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/spark-257ecfa2-0753-4c68-972b-9b7f5b2e5830/pyspark-7c96bc05-107f-485a-b134-a266137d2f9c
20/11/22 03:53:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/5h/4wpb8p7n00b1r_r5d2h0y__r0000gn/T/spark-257ecfa2-0753-4c68-972b-9b7f5b2e5830
